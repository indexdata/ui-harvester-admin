Help texts from `localindices/harvester-admin/src/main/resources/contexthelp.properties`

Those indented by a space have been incorportated into `../translations/ui-harvester-admin/en.json`.

Check the texts for:
* HTML tags
* &entities;
* \u1234

Harvest_Jobs.Jobs_list.Filter = Criteria to filter the jobs list by. Pressing Enter will activate the filter. The page will include jobs that has the entered text in one of the fields : <ul> <li>Name</li> <li>Description</li> <li>Technical Notes</li> <li>Contact Notes</li> <li>Service Provider</li> <li>Usage tags</li> <li>Admin tags</li> <li>Current Status</li> </ul>
 Harvest_Jobs.General_information.Id   =  Automatically assigned identifier for the job
 Harvest_Jobs.General_information.Name =  Preferably a unique name for users to identify this Harvester  resource. In some cases the name may be proposed after filling  out protocol specific section of the configuration (e.g Index  Data Connectors, OAI-PMH).
 Harvest_Jobs.General_information.Service_Provider = Free-text field used by support staff for  recording administrative information.<br/> Not used by the harvester.
 Harvest_Jobs.General_information.Used_by = Free-text field for tagging a job  with the intended target audience, like a user group  or customer of the resource.<br/>  Multiple user/customer tags may be separated by commas.<br/> The tags can be used for filtering status reports by usages/customers.  
 Harvest_Jobs.General_information.Managed_by = Free-text field for tagging a job  with the producer or manager of the resource.<br/>  Multiple tags may be separated by commas.<br/> The tags can be used for filtering status reports by job  administrators.  
 Harvest_Jobs.General_information.Content_Description = Free-text field used by support staff  for recording administrative information. Not used by the harvester.
 Harvest_Jobs.General_information.Technical_Notes =  Free-text field used by support staff for  recording administrative information. Not used by the harvester.
 Harvest_Jobs.General_information.Contact_Notes = Free-text field used by support staff for recording  administrative information. Not used by the harvester.
 Harvest_Jobs.General_information.Harvest_job_enabled = Check to run the Harvesting job as described by the  time/interval selected in "Harvest schedule". Leaving this  box unchecked will make the job inactive.
 Harvest_Jobs.General_information.Harvest_schedule = Use these fields to define a recurring time/interval at  which the Harvester job should run. E.g  for weekly runs specify a day of the week on which the harvest  should be executed.
 Harvest_Jobs.General_information.Transformation_Pipeline = Select the transformation required to match the input  format delivered by the feed to the internal format used by the Harvester for  data storage. See the Transformation Pipelines manual section for more details.
 Harvest_Jobs.General_information.Use_lax_parsing_(if_possible) = When enabled, harvester will attempt to parse malformed XML  (missing closing tags, entities)
 Harvest_Jobs.General_information.Encoding_override_(ISO-8859-1,_UTF-8,_...) = A feed can return invalid encoded responses, such as  having an XML header with encoding set to UTF-8, but actually return ISO-8859-1 in  the data. Setting this field to the actual encoding will force the Harvester to use  the specified encoding.
 Harvest_Jobs.General_information.Storage = Select the storage type and location for the harvested data. The Harvester  has a storage abstraction layer to allow it to work with multiple potential  record storage systems, but at present, only Solr/Lucene is supported. <br/><br/>Once  the Storage has been selected, it is possible to view the indexed records by  clicking the Stored records: click to view field.
Harvest_Jobs.General_information.Stored_records = Follow this link to view previously indexed records from the chosen Storage.
 Harvest_Jobs.General_information.Cache_on_disk = If enabled, harvest data is kept in the filesystem cache and the job  can be restarted from this cache without needing to go back to the server.
 Harvest_Jobs.General_information.Limit_record_number_to = Limit the harvest run to a specified number of records:  useful for testing job settings and transformation pipelines.
 Harvest_Jobs.General_information.Connection/read_timeout_(seconds) = Specify a non-default timeout value for obtaining and  reading from the network connection (socket). Values under 1 minute are not  recommended.
 Harvest_Jobs.General_information.Log_level = Specify the logging level for the job with DEBUG being the most verbose.  INFO is the recommended log level in most cases.
 Harvest_Jobs.General_information.Saving_failed_records = Specify whether or not failed records should be saved as XML files in a  designated log directory. Also specify retention policy for the directory, that is, whether  to retain files that were saved in previous runs and, if so, whether to overwrite an existing  file if the same record fails again or rather add a sequence number to the new file name in order  not to overwrite.
 Harvest_Jobs.General_information.Maximum_number_of_failed_records_saved_next_run = Sets a maximum number of files to save in  the failed records directory per run. The job log will tell when the limit is reached.
 Harvest_Jobs.General_information.Maximum_number_of_failed_records_saved_total = Sets a maximum number of files to be saved  in the failed records directory at any given time - as the sum of previously saved  records (that were not cleaned up before this run) plus any new records added during the run. The job log will tell when the limit is reached.
 Harvest_Jobs.General_information.Notification_e-mail_address(es)_(separate_with_comma) = Specify comma separated list of e-mail addresses that  should receive notification on job completion.
 Harvest_Jobs.General_information.Extra_configuration_(JSON) = Specify additional advanced harvester configuration in the  JSON format.
 Harvest_Jobs.General_information.Send_notification_if_severity_at_least = specify job completion status with the least  severity that will trigger the e-mail notification.
 Harvest_Jobs.General_information.List_of_constant_fields = a comma-separated list of NAME=VALUE pairs. For a harvestable that has this field set, each harvested record has each NAME field set to the corresponding VALUE.
 Harvest_Jobs.OAI-PMH_specific_information.OAI_Repository_URL = Enter a link (http-based) to the resource to be harvested.  Include the base link defined by OAI Set Name: (see below). Some resources have  multiple sets within the repository. If no specific set is identified by the URL,  the full repository will be harvested.
 Harvest_Jobs.OAI-PMH_specific_information.OAI_Set_Name_(type_for_suggestions) = an optional setting, an OAI-PMH setSpec value which  specifies set criteria for selective harvesting.
 Harvest_Jobs.OAI-PMH_specific_information.Metadata_Prefix = A string that specifies the metadata format in OAI-PMH requests  issued to a targeted repository. It is important to choose the correct format or no  data will be harvested from the repository. Make sure a Transformation Pipeline that  matches the metadata format used in the repository is selected, otherwise records  will not be understood by the Harvester. Repositories generally use one of the  following prefixes (or embedded data formats): Dublin Core (OAI-DC) or MARC XML  (MARC12/USMARC). Other less common MetadataPrefix values include PMC (PubMed Central  full-text records), PMC (PubMed Central metadata records), and PZ2 (pazpar2).
 Harvest_Jobs.OAI-PMH_specific_information.Use_long_date_format = Check-box to indicate whether to use a long date format when  requesting records from the OAI-PMH resource. This is not used very often, but is  required by some resources.
 Harvest_Jobs.OAI-PMH_specific_information.Harvest_from_(yyyy-MM-dd) = If empty and no resumption token is set, the Harvester  will harvest the full data set from the resource. When this field contains a value,  upon completion of the job the Harvester will reset the value of this field to the day  prior to the current run date, so subsequent runs will harvest only new records.
 Harvest_Jobs.OAI-PMH_specific_information.Harvest_until_(yyyy-MM-dd) = Upper date limit for selective harvesting. On consecutive  runs the Harvester will clear this field making the date interval open-ended.
 Harvest_Jobs.OAI-PMH_specific_information.Resumption_token_(overrides_date) = The OAI-PMH protocol supports splitting bigger  datasets into smaller chunks. On delivery of a chunk of records, the OAI-PMH returns a  token which the next request should use in order to get the next chunk. If an OAI-PMH  job halts before completion, the resumption token will be set in this field. Sometimes  it is possible to run it again from this resumption point at a later stage, but this  is not always supported.
 Harvest_Jobs.OAI-PMH_specific_information.Clear_resumption_token_on_connection_errors = Clear the resumption token for  harvests that complete in an error state. This is useful when server errors out and  the last resumption token is no longer valid.
 Harvest_Jobs.OAI-PMH_specific_information.Keep_partial_harvests = When checked, partial records harvested during a failed  harvest run will be retained in storage.
 Harvest_Jobs.OAI-PMH_specific_information.Request_retry_count = Specify how many times the harvester should retry failed  harvest requests, 0 disables retrying entirely.
 Harvest_Jobs.OAI-PMH_specific_information.Delay_before_retry_(seconds) = Delay for retrying failed requests. Only change  when resource fails to work with the default values.
 Harvest_Jobs.XML_bulk_specific_information.URLs_(space-separated) = One or more space-separated URL (HTTP or FTP) for XML  or MARC binary data. Jump or index pages (HTML pages with URLs) are supported and  so are FTP directories. For FTP, harvesting of recursive directories may be enabled below.
 Harvest_Jobs.XML_bulk_specific_information.Continue_on_errors = Check to continue harvesting and storing records even if  retrieving some of the listed resources fails.
 Harvest_Jobs.XML_bulk_specific_information.Overwrite_data_with_each_run_(non-incremental) = Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run. This  may be used when complete catalog dumps reside on the server. <br/><br/> With FOLIO Inventory Storage there is no deletion of all previously harvested data, and checking  this option instead indicates that existing records should be overlaid. 
 Harvest_Jobs.XML_bulk_specific_information.Ask_server_for_new_files_only_(incremental) = Ask the server if the files are  modified before attempting a harvest, relies on proper timestamp handling on the  server side. It\u2019s usually safe to have this enabled as servers are eager to  update the modification date, even in cases when the files themselves don\u2019t  change. Enabling this setting may significantly shorten harvest times.
 Harvest_Jobs.XML_bulk_specific_information.Initial_from_date = Allows to specify the initial from  harvest date when <strong>ask server for new files only</strong> option is checked. When filled out, only files newer than the specified date will be harvested. <br/><br/> With FOLIO Inventory Storage, the setting additionally indicates that only incoming records  that were updated on or after this date should be loaded. Additionally, for this to take effect,  the incoming records must provide a 'lastUpdated' in the element 'processing' and on the format YYYY-MM-DD  &lt;processing&gt; &lt;lastUpdated&gt;1970-01-01&lt;/lastUpdated&gt; &lt;/processing&gt; By default the logic would filter by the finishing date of the last harvest, so setting  'Initial from date' overrides the default behavior.<br/><br/> Following rules thus applies: <br/> <li>If 'Overwrite data' is checked, all records are loaded.</li> <br/>Otherwise:<br/><br/> <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is set,  then only records updated at or after that date will be loaded (this is regardless of  whether 'Ask server for new files only (incremental)' is checked or not)</li> <li>If the incoming records provide a 'lastUpdated' date, and the 'Initial from date' is NOT set,  then only records updated at or after the last harvest date will be loaded</li> <br/><br/>But<br/><br/> <li>Any record without a 'lastUpdated' date will be loaded</li>
 Harvest_Jobs.XML_bulk_specific_information.Split_XML_at_depth_(zero/empty_disables_split) = For XML data. This should  usually be set to 1 for XML feeds, if we want to harvest the record elements in  the data structured like:<br/><br/> &lt;root&gt;<br/>&nbsp;&lt;record/&gt;<br/>&nbsp;&lt;record/&gt;<br/>&lt;/root&gt;
 Harvest_Jobs.XML_bulk_specific_information.Split_files_at_number_of_records_(zero/empty_disables_split) = The Harvester  tries to imply streaming parsing where possible, but many XSL Transformations  will not support this. Attempting to transform millions of records will be too  memory consuming, so breaking the resource into chunks of 1000 records seems to  be a reasonable option. Enter into this field the number of records to be  contained in each chunk.
 Harvest_Jobs.XML_bulk_specific_information.Mime-type_override_(e.g_application/marc;_charsetMARC-8) = The Harvester detects  the type (XML vs MARC binary) from the MIME-type and file extension. It is also able  to deal with compressed archives (zip, tar, gzip), in some rare case it may be  required to provide the content type manually (e.g if it\u2019s missing or wrong),  the format is:<br/><br/> MIME-type [; optional character encoding].
 Harvest_Jobs.XML_bulk_specific_information.MARC_XML_transformation_format_(application/marc_or_application/tmarc) =  This field  expresses the output format of binary MARC reading\u2013which will also be the input  format for the transformation pipeline. If the Transformation Pipeline expects MARC21  XML, this should be set to Application/marc. If the pipeline expects Turbo MARC XML,  it should be set to Application/tmarc.
 Harvest_Jobs.XML_bulk_specific_information.Recurse_into_subfolders = When set, the harvester will traverse the entire directory  tree and search for harvestable files. This setting should be enabled with care.
 Harvest_Jobs.XML_bulk_specific_information.Include_files_(regular_expression) = This setting can be used to filter what files  to harvest.  The filter applies to files in FTP directories as well as in archives (ZIP/tar).  When set to a regular expression, the harvester  will only harvest files with names matching the regular expression (unless the file name is  at the same time excluded by an exclude pattern).<br/><br/> Example:  <pre>.*\\.xml|.*\\.marc</pre>  Would include only .xml and .marc files.  <br/><br/>Note that file name dots must be escaped.  <br/><br/>Note that ZIP and tar files (.zip,.gz,.tar) are loaded even if they are not specified in the  include pattern. To enforce exclusion of ZIP or tar files they would have to be specified in  an exclude pattern (see help text for that).
 Harvest_Jobs.XML_bulk_specific_information.Exclude_files_(regular_expression) = This setting can be used to filter what files to harvest. The filter applies to files in FTP directories as well as entries in archives (ZIP/tar).  When set to a regular expression, the harvester  will skip any file with a file name matching the expression. Example: <pre>readme\\.txt|README|.*\\.jpg|.*\\.gif</pre>  Would exclude files with names readme.txt, README as well as .jpg and .gif files  from FTP directories or ZIP/tar archives.
 Harvest_Jobs.XML_bulk_specific_information.Use_passive_mode_for_FTP_transfers = When set passive, instead of active, mode is  used for FTP connections. If harvester is running within a restricted firewall that  blocks FTP active mode connections, enabling this setting might help. It might be,  however, necessary to align this mode with what FTP server expects.
 Harvest_Jobs.XML_bulk_specific_information.CSV_parser_configuration = the harvester will detect (either by MIME-type or by file extension)  and attempt to parse CSV (comma separated values) files into an XML representation for further processing. The XML representation of each  data row looks as follows: <pre>&lt;row&gt;&lt;field name\=&quot;column name or number&quot;&gt;field value&lt;/field&gt;...&lt;/row&gt;</pre>  Unless the split at depth option is set to > 0, all rows will be parsed into a single XML document and wrapped with an additional <rows>  root element. For large CSV files it may be a good idea to set the split at depth to 1.  The parser configuration is expressed in a semicolon delimited key/value list, like so: key1\=value1; key2\=value2. List of supported options is as follows\: <ul> <li> charset\: default "iso-8859-1", specifies the character encoding of the files <li> delimiter\: default "," for CSV and "\t" for TSV, specifies the field delimiter used in the files <li> containsHeader\: default "yes", specifies if the first line in the files contains the header line <li> headerLine\: no default, allows to override or specify headers, format is a comma-separated list e.g headers="title,author,description" <ul>
 Harvest_Jobs.Connector_specific_information.CF_Engine = Select the Connector Engine instance that will be used to execute  the Connector harvesting job. The default engine is hosted by Index Data but may be  also installed locally on the customer site. Additional Connector Engines can be  specified through the Settings tab.
 Harvest_Jobs.Connector_specific_information.Engine_parameters_(optional) = Additional or custom values of Connector Engine  session parameters used by this job. See CFWS manpage for more information.
 Harvest_Jobs.Connector_specific_information.CF_Repository = Select the connector repository where the Connectors are hosted  and maintained. Usually, the Connector Repository is provided by Index Data and may  require a login account. The account credentials are provided directly in the  Connector Repository URL setting accessed from the Settings tab and should have the  form: <pre>http(s)://&lt;repouser&gt;:&lt;repopass&gt;@url.to.the.repository</pre>.
 Harvest_Jobs.Connector_specific_information.Connector_(type_for_suggestion) = Enter here the name of the harvesting  connector specific to the harvested resource. This field provides suggestions by  looking up the Repository so only a couple of initial characters or a part of the  name is required.
 Harvest_Jobs.Connector_specific_information.Overwrite_with_each_run_(non-incremental) = Check to delete all previously  harvested data before beginning the next scheduled (or manually triggered) run.
 Harvest_Jobs.Connector_specific_information.User_Name = User name required for access to a harvested resource that  requires authentication.
 Harvest_Jobs.Connector_specific_information.Password = Password required for access to a harvested resource that requires  authentication.
 Harvest_Jobs.Connector_specific_information.Proxy_server_address = Address of the proxy server that should be used by the  harvesting engine, e.g to deal with cases when the resource is IP authenticated.
 Harvest_Jobs.Connector_specific_information.Init_Data = Advanced setting to provide additional initialization parameters  to the harvesting connector. Any username/password/proxy specified in the inputs  above will take precedence over settings specified in this field. These settings  must be provided in JSON format.
Harvest_Jobs.Connector_specific_information.Harvest_from_(yyyy-MM-dd) = Start date for selective harvesting; this functionality  depends on the connector capability.
Harvest_Jobs.Connector_specific_information.Harvest_until_(yyyy-MM-dd) = End date for selective harvesting; this functionality  depends on the connector capability
 Harvest_Jobs.Connector_specific_information.Start_token_(incremental_harvest) = The use of a start token for incremental  harvesting is connector specific and depends on the connector capability. This setting  must be provided in JSON format.
 Harvest_Jobs.Connector_specific_information.Delay_between_requests_(milliseconds) = Delay between requests made from the  harvester to the connector engine. Use when the resource is sensitive to high loads.
 Harvest_Jobs.Connector_specific_information.Failed_request_retry_count = Specify how many times the harvester should retry  failed harvest requests, 0 disables retrying entirely.
Harvest_Jobs.Connector_specific_information.Continue_on_errors = If checked the harvester will ignore failed harvest requests  (subject to retry count) and continue until link tokens are exhausted. This may lead  to partial harvests.
 Harvest_Jobs.Status_job_information.Filter,_comma-separated_list_of_usage_tags = Used for filtering the status report by the user groups on customers tagged to a harvest job ("Used by").
 Harvest_Jobs.Status_job_information.Filter,_comma-separated_list_of_admin_tags = Used for filtering the status report by the harvest jobs creator or administrator, as tagged to the harvest job ("Managed by")
Storage_Engines.General_information.Name = Uniquely identifiable name, for example, \u201cSolr (tomcat) @ donut\u201d
Storage_Engines.General_information.Storage_Description = Optional description, such as \u201cSolr running in tomcat on donut\u201d
Storage_Engines.General_information.Enabled = The Storage element will only be available for new Harvester jobs when enabled.
Storage_Engines.General_information.Server_URL = The web service end point, e.g., http:///solr/
Storage_Engines.FOLIO_Inventory_specific_information.Server_URL = The URL of the FOLIO Okapi service providing the Inventory storage, e.g. http://folio:9130/  (address must end with a slash as shown)
Storage_Engines.FOLIO_Inventory_specific_information.Storage_configuration_(JSON) = Configuration in JSON:  specify FOLIO module end-points and credentials <p><b>Template: Updating Inventory storage while matching by HRID - source record storage not used</b></p> <pre> {\n   "folioAuthPath": "bl-users/login",\n   "folioTenant": [put tenant here - ie "diku"],\n   "folioUsername": [put FOLIO user name here - ie "diku_admin"],\n   "folioPassword": [put FOLIO password here - ie "admin"],\n   "inventoryUpsertPath": "inventory-upsert-hrid"\n   }\n </pre> <p><b>Template: Updating shared Inventory, matching by match key - with MARC record storage</b></p> <pre> {\n   "folioAuthPath": "bl-users/login",\n   "folioTenant": [put tenant here - ie "diku"],\n   "folioUsername": [put FOLIO user name here - ie "diku_admin"],\n   "folioPassword": [put FOLIO password here - ie "admin"],\n   "inventoryUpsertPath": "shared-inventory-upsert-matchkey",\n   "marcStoragePath": "marc-records"\n } </pre>
Transformation_Pipelines.General_information.Name = Uniquely identifiable name that describes the transformation, e.g.  \u201cOAI-PMH(DC) to PZ (medium book)\u201d
Transformation_Pipelines.General_information.Description = Optional description of the details of the transformation, such as  \u201cConverting from OAI-PMH(DC) to PZ\u201d.
Transformation_Pipelines.General_information.Enabled = Check to enable the transformation pipeline.
Transformation_Pipelines.General_information.Parallel_(run_each_step_in_own_thread) =
Transformation_Pipelines.General_information.Transformation/Configuration = Valid XSL transformation script. Note that XSLT  up to version 2 is supported. When the transformation is provided, you can click the Check button to  validate and compile the transform, any errors will be displayed in red.
Transformation_Pipelines.General_information.Test = If you provide some test data in this input area and then click the Run  button, you can see the result (or an error message in red) of the transformation.
Transformation_Pipelines.General_information.Output = Output of the test run.
Transformation_Steps.General_information.Step_Name = A unique, descriptive name
Transformation_Steps.General_information.Description = Description of what the step does
Transformation_Steps.General_information.Input_format = In a future release, this field will be used for automated  filtering and validation. Currently, this field is used for visual step  validation when viewed as a list. As shown in the illustration below,  steps viewed in a series must display the output type in a previous step  corresponding to the input format in a succeeding step, otherwise the  transformation will fail.
Transformation_Steps.General_information.Output_format = In a future release, this field will be used for automated  filtering and validation. Currently, this field is used for visual step  validation when viewed as a list. As shown in the illustration below,  steps viewed in a series must display the output type in a previous step  corresponding to the input format in a succeeding step, otherwise the  transformation will fail
Transformation_Steps.General_information.Transformation/Configuration = A valid XSL transformation script. Note that  XSLT up to version 2 is supported. When the transformation is provided, you can  click the Check button to validate and compile the transform, any errors will be  displayed in red.
Transformation_Steps.General_information.Test = If you provide some test data in this input area and then click the  Run button, you can see the result (or an error message in red) of the transformation.
Transformation_Steps.General_information.Output = Output of the rest run.
